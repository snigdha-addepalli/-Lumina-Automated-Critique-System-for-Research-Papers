{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T19:54:38.064934Z",
     "iopub.status.busy": "2025-04-15T19:54:38.064636Z",
     "iopub.status.idle": "2025-04-15T19:54:44.317963Z",
     "shell.execute_reply": "2025-04-15T19:54:44.317073Z",
     "shell.execute_reply.started": "2025-04-15T19:54:38.064911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ArXiv Dataset Generation with Llama-2-7B for Research Paper Critiques\n",
    "# # This script downloads real papers from arXiv and uses Llama-2 to create a critique dataset\n",
    "\n",
    "# Install required packages - minimizing dependencies\n",
    "!pip install -q arxiv==1.4.7 pymupdf==1.23.5 datasets==2.14.5 tqdm==4.66.1\n",
    "!pip install -q torch==2.1.0 transformers==4.36.0 accelerate==0.25.0 bitsandbytes==0.41.0\n",
    "\n",
    "# # Import necessary libraries\n",
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import random\n",
    "# import time\n",
    "# import arxiv\n",
    "# import fitz  # PyMuPDF\n",
    "# import torch\n",
    "# from tqdm.notebook import tqdm\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# # Create directories\n",
    "# !mkdir -p ./data\n",
    "# !mkdir -p ./papers\n",
    "\n",
    "# # Function to download papers from arXiv\n",
    "# def download_arxiv_papers(search_queries, max_results=10, output_dir=\"./papers\"):\n",
    "#     \"\"\"Download papers from arXiv using the API.\"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     downloaded_papers = []\n",
    "    \n",
    "#     for query in search_queries:\n",
    "#         print(f\"Searching arXiv for: {query}\")\n",
    "        \n",
    "#         # Create a search client\n",
    "#         search = arxiv.Search(\n",
    "#             query=query,\n",
    "#             max_results=max_results,\n",
    "#             sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "#             sort_order=arxiv.SortOrder.Descending\n",
    "#         )\n",
    "        \n",
    "#         # Download papers\n",
    "#         for result in tqdm(search.results(), desc=f\"Downloading papers for '{query}'\"):\n",
    "#             # Create a safe filename\n",
    "#             paper_id = result.get_short_id()\n",
    "#             filename = f\"{paper_id}.pdf\"\n",
    "#             filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "#             # Skip if already downloaded\n",
    "#             if os.path.exists(filepath):\n",
    "#                 downloaded_papers.append(filepath)\n",
    "#                 continue\n",
    "            \n",
    "#             try:\n",
    "#                 # Download the paper\n",
    "#                 result.download_pdf(dirpath=output_dir, filename=filename)\n",
    "#                 downloaded_papers.append(filepath)\n",
    "                \n",
    "#                 # Be nice to arXiv API\n",
    "#                 time.sleep(3)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error downloading {paper_id}: {str(e)}\")\n",
    "    \n",
    "#     return downloaded_papers\n",
    "\n",
    "# # Function to extract paragraphs from PDFs\n",
    "# def extract_paragraphs_from_pdfs(pdf_paths, min_words=50, max_words=300):\n",
    "#     \"\"\"Extract paragraphs from PDF files.\"\"\"\n",
    "#     all_paragraphs = []\n",
    "    \n",
    "#     for pdf_path in tqdm(pdf_paths, desc=\"Processing PDFs\"):\n",
    "#         try:\n",
    "#             # Open the PDF\n",
    "#             doc = fitz.open(pdf_path)\n",
    "            \n",
    "#             # Detect the starting page (skip front matter)\n",
    "#             start_page = 0\n",
    "#             for i in range(min(5, len(doc))):\n",
    "#                 text = doc[i].get_text().lower()\n",
    "#                 if any(marker in text for marker in [\"introduction\", \"background\", \"1. introduction\"]):\n",
    "#                     start_page = i\n",
    "#                     break\n",
    "            \n",
    "#             # Extract text from each page\n",
    "#             for page_num in range(start_page, len(doc)):\n",
    "#                 page = doc[page_num]\n",
    "#                 blocks = page.get_text(\"blocks\")  # Get text as blocks which preserves layout better\n",
    "                \n",
    "#                 for block in blocks:\n",
    "#                     # Block structure: (x0, y0, x1, y1, text, block_type, block_no)\n",
    "#                     text = block[4].strip()\n",
    "                    \n",
    "#                     # Clean up the text\n",
    "#                     text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "                    \n",
    "#                     # Skip if empty or too short\n",
    "#                     if not text or len(text.split()) < min_words:\n",
    "#                         continue\n",
    "                        \n",
    "#                     # Skip likely headers, footers, figure captions, etc.\n",
    "#                     if re.match(r'^[0-9]+\\.?\\s*$', text):  # Just a number\n",
    "#                         continue\n",
    "#                     if re.match(r'^figure\\s+[0-9]+', text.lower()):  # Figure caption\n",
    "#                         continue\n",
    "#                     if re.match(r'^table\\s+[0-9]+', text.lower()):  # Table caption\n",
    "#                         continue\n",
    "#                     if text.isupper() and len(text.split()) < 15:  # ALL CAPS header\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Handle paragraphs that are too long\n",
    "#                     if len(text.split()) > max_words:\n",
    "#                         words = text.split()\n",
    "#                         chunks = []\n",
    "#                         for i in range(0, len(words), max_words):\n",
    "#                             chunk = ' '.join(words[i:i+max_words])\n",
    "#                             chunks.append(chunk)\n",
    "#                         all_paragraphs.extend(chunks)\n",
    "#                     else:\n",
    "#                         all_paragraphs.append(text)\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "    \n",
    "#     print(f\"Extracted {len(all_paragraphs)} paragraphs from {len(pdf_paths)} PDFs\")\n",
    "#     return all_paragraphs\n",
    "\n",
    "# # Function to set up Llama-2 model\n",
    "# def setup_llama_model(device=\"cuda\", load_in_8bit=True):\n",
    "#     \"\"\"Set up the Llama-2 model for text generation.\"\"\"\n",
    "#     print(\"Loading Llama-2 7B Chat model...\")\n",
    "    \n",
    "#     # Configure quantization if using CUDA\n",
    "#     if device == \"cuda\" and torch.cuda.is_available() and load_in_8bit:\n",
    "#         bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_8bit=True,\n",
    "#             bnb_8bit_use_double_quant=True,\n",
    "#             bnb_8bit_quant_type=\"nf4\",\n",
    "#             bnb_8bit_compute_dtype=torch.float16\n",
    "#         )\n",
    "        \n",
    "#         # Load tokenizer and model - Using Meta AI's open source version\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#             device_map=\"auto\",\n",
    "#             quantization_config=bnb_config,\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "#     else:\n",
    "#         # CPU or non-quantized loading\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#             device_map=\"auto\" if device == \"cuda\" else None,\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "    \n",
    "#     # Set padding token if not set\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "#     return tokenizer, model\n",
    "\n",
    "# # Function to format Llama-2 prompt\n",
    "# def format_llama_prompt(instruction, content=None):\n",
    "#     \"\"\"Format a prompt according to Llama-2's expected format.\"\"\"\n",
    "#     if content:\n",
    "#         return f\"<s>[INST] {instruction}\\n\\n{content} [/INST]\"\n",
    "#     else:\n",
    "#         return f\"<s>[INST] {instruction} [/INST]\"\n",
    "\n",
    "# # Function to extract Llama-2 response\n",
    "# def extract_llama_response(generated_text):\n",
    "#     \"\"\"Extract the model's response from the generated text.\"\"\"\n",
    "#     response = generated_text.split(\"[/INST]\")[-1].strip()\n",
    "#     if \"</s>\" in response:\n",
    "#         response = response.split(\"</s>\")[0].strip()\n",
    "    \n",
    "#     return response\n",
    "\n",
    "# # Function to generate a critique for a paragraph\n",
    "# def generate_critique(tokenizer, model, paragraph, issue_type=None):\n",
    "#     \"\"\"Generate a critique for a paragraph using Llama-2.\"\"\"\n",
    "#     # Define issue types\n",
    "#     issue_types = [\n",
    "#         \"missing_evidence\",\n",
    "#         \"logical_contradiction\",\n",
    "#         \"unclear_argument\",\n",
    "#         \"poor_citation\",\n",
    "#         \"grammar_spelling\",\n",
    "#         \"undefined_terminology\",\n",
    "#         \"statistical_error\",\n",
    "#         \"methodology_issue\",\n",
    "#         \"unsubstantiated_claim\",\n",
    "#         \"structural_issue\",\n",
    "#         \"well_written\"  # Include examples with no issues\n",
    "#     ]\n",
    "    \n",
    "#     # Select a random issue type if not specified\n",
    "#     if issue_type is None:\n",
    "#         issue_type = random.choice(issue_types)\n",
    "    \n",
    "#     # Create instruction\n",
    "#     instruction = f\"\"\"You are an expert academic reviewer with years of experience reviewing research papers.\n",
    "# Analyze the following paragraph from a real research paper and provide a detailed critique.\n",
    "# Focus on identifying issues related to: {issue_type.replace('_', ' ')}.\n",
    "# If you genuinely find no issues, explain why the paragraph is well-written instead.\n",
    "# Your critique should be specific, actionable, and professional.\n",
    "\n",
    "# Provide only the critique - do not include any introductory text or explanations about your role.\"\"\"\n",
    "    \n",
    "#     # Format prompt\n",
    "#     prompt = format_llama_prompt(instruction, paragraph)\n",
    "    \n",
    "#     try:\n",
    "#         # Tokenize input\n",
    "#         inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#         inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "#         # Generate text\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=512,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             do_sample=True,\n",
    "#             pad_token_id=tokenizer.pad_token_id\n",
    "#         )\n",
    "        \n",
    "#         # Decode output\n",
    "#         generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        \n",
    "#         # Extract response\n",
    "#         critique = extract_llama_response(generated_text)\n",
    "        \n",
    "#         # Create result\n",
    "#         result = {\n",
    "#             \"paragraph\": paragraph,\n",
    "#             \"critique\": critique,\n",
    "#             \"issue_type\": issue_type\n",
    "#         }\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error generating critique: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# # Function to create dataset\n",
    "# def create_dataset(paragraphs, tokenizer, model, num_examples, output_path=\"./data\"):\n",
    "#     \"\"\"Create a dataset of paragraph-critique pairs.\"\"\"\n",
    "#     examples = []\n",
    "    \n",
    "#     # Ensure we don't try to generate more examples than paragraphs\n",
    "#     num_examples = min(num_examples, len(paragraphs))\n",
    "    \n",
    "#     # Shuffle paragraphs\n",
    "#     random.shuffle(paragraphs)\n",
    "    \n",
    "#     # Create progress bar\n",
    "#     with tqdm(total=num_examples, desc=\"Generating critiques\") as pbar:\n",
    "#         for i in range(num_examples):\n",
    "#             # Get paragraph\n",
    "#             paragraph = paragraphs[i]\n",
    "            \n",
    "#             # Generate critique\n",
    "#             example = generate_critique(tokenizer, model, paragraph)\n",
    "            \n",
    "#             if example:\n",
    "#                 examples.append(example)\n",
    "#                 pbar.update(1)\n",
    "            \n",
    "#             # Clear CUDA cache occasionally\n",
    "#             if i % 10 == 9:\n",
    "#                 torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Create output directory\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "#     # Convert to HuggingFace Dataset\n",
    "#     dataset = Dataset.from_list(examples)\n",
    "    \n",
    "#     # Split into train/validation sets (90/10)\n",
    "#     splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "#     # Save the dataset\n",
    "#     splits[\"train\"].to_json(os.path.join(output_path, \"train_data.json\"))\n",
    "#     splits[\"test\"].to_json(os.path.join(output_path, \"val_data.json\"))\n",
    "    \n",
    "#     # Format for different models\n",
    "#     format_for_models(splits, output_path)\n",
    "    \n",
    "#     print(f\"Dataset saved to {output_path}\")\n",
    "#     print(f\"Train set: {len(splits['train'])} examples\")\n",
    "#     print(f\"Validation set: {len(splits['test'])} examples\")\n",
    "    \n",
    "#     return splits\n",
    "\n",
    "# # Function to format for different models\n",
    "# def format_for_models(dataset, output_path):\n",
    "#     \"\"\"Format dataset for different model types.\"\"\"\n",
    "#     # Format for Mistral\n",
    "#     formatted_mistral_train = []\n",
    "#     for example in dataset[\"train\"]:\n",
    "#         formatted_mistral_train.append({\n",
    "#             \"messages\": [\n",
    "#                 {\"role\": \"user\", \"content\": f\"Review and critique the following research paper paragraph. Identify any logical issues, missing evidence, contradictions, or other problems:\\n\\n{example['paragraph']}\"},\n",
    "#                 {\"role\": \"assistant\", \"content\": example['critique']}\n",
    "#             ]\n",
    "#         })\n",
    "    \n",
    "#     formatted_mistral_val = []\n",
    "#     for example in dataset[\"test\"]:\n",
    "#         formatted_mistral_val.append({\n",
    "#             \"messages\": [\n",
    "#                 {\"role\": \"user\", \"content\": f\"Review and critique the following research paper paragraph. Identify any logical issues, missing evidence, contradictions, or other problems:\\n\\n{example['paragraph']}\"},\n",
    "#                 {\"role\": \"assistant\", \"content\": example['critique']}\n",
    "#             ]\n",
    "#         })\n",
    "    \n",
    "#     # Save Mistral format\n",
    "#     with open(os.path.join(output_path, \"train_mistral_format.json\"), \"w\") as f:\n",
    "#         json.dump(formatted_mistral_train, f, indent=2)\n",
    "    \n",
    "#     with open(os.path.join(output_path, \"val_mistral_format.json\"), \"w\") as f:\n",
    "#         json.dump(formatted_mistral_val, f, indent=2)\n",
    "    \n",
    "#     print(f\"Model-specific formatted data saved to {output_path}\")\n",
    "\n",
    "# # MAIN EXECUTION\n",
    "\n",
    "# # Check if we need to use a token for Llama-2 access\n",
    "# import getpass\n",
    "# print(\"Note: You'll need a Hugging Face token with access to Llama-2 to use this script.\")\n",
    "# print(\"If you don't have one, consider using another model like TinyLlama or Phi-2.\")\n",
    "\n",
    "# try_llama = input(\"Do you want to try using Llama-2? (y/n): \")\n",
    "\n",
    "# if try_llama.lower() == 'y':\n",
    "#     # 1. Define search queries for arXiv\n",
    "#     search_queries = [\n",
    "#         \"cat:cs.AI\",  # Artificial Intelligence\n",
    "#         \"cat:cs.CL\",  # Computational Linguistics\n",
    "#         \"cat:cs.LG\",  # Machine Learning\n",
    "#         \"cat:cs.CV\",  # Computer Vision\n",
    "#         \"cat:cs.NE\",  # Neural and Evolutionary Computing\n",
    "#         \"cat:cs.IR\",  # Information Retrieval\n",
    "#         \"cat:stat.ML\",  # Machine Learning (Statistics)\n",
    "#         \"cat:cs.RO\"   # Robotics\n",
    "#     ]\n",
    "\n",
    "#     # Ask user for number of papers to download per category\n",
    "#     num_papers = int(input(\"Enter number of papers to download per category (5-10 recommended): \"))\n",
    "#     print(f\"Will download approximately {num_papers} papers per category...\")\n",
    "\n",
    "#     # 2. Download papers from arXiv\n",
    "#     pdf_paths = download_arxiv_papers(search_queries, max_results=num_papers)\n",
    "#     print(f\"Downloaded {len(pdf_paths)} papers\")\n",
    "\n",
    "#     # 3. Extract paragraphs from PDFs\n",
    "#     paragraphs = extract_paragraphs_from_pdfs(pdf_paths)\n",
    "\n",
    "#     # 4. Set up Llama-2 model\n",
    "#     try:\n",
    "#         tokenizer, model = setup_llama_model()\n",
    "\n",
    "#         # 5. Ask user for dataset size\n",
    "#         num_examples = int(input(f\"Found {len(paragraphs)} paragraphs. How many examples to generate? \"))\n",
    "#         num_examples = min(num_examples, len(paragraphs))\n",
    "#         print(f\"Will generate {num_examples} examples...\")\n",
    "\n",
    "#         # 6. Create dataset\n",
    "#         dataset_splits = create_dataset(paragraphs, tokenizer, model, num_examples)\n",
    "\n",
    "#         # 7. Print information about saving\n",
    "#         print(\"\\nDataset generation complete!\")\n",
    "#         print(\"You can now save this dataset to your Kaggle account:\")\n",
    "#         print(\"1. Click on the 'Data' tab in the right panel\")\n",
    "#         print(\"2. Under 'Output', click '+ Save All'\")\n",
    "#         print(\"3. Enter a name for your dataset (e.g., 'research-paper-critique-data')\")\n",
    "#         print(\"4. Click 'Save'\")\n",
    "#         print(\"\\nYou can then use this dataset in a new notebook for fine-tuning Mistral\")\n",
    "\n",
    "#         # Display a sample from the dataset\n",
    "#         print(\"\\nHere's a sample from the generated dataset:\")\n",
    "#         with open(\"./data/train_data.json\", \"r\") as f:\n",
    "#             data = json.load(f)\n",
    "#             sample = data[0]\n",
    "#             print(\"\\nPARAGRAPH:\")\n",
    "#             print(sample[\"paragraph\"])\n",
    "#             print(\"\\nCRITIQUE:\")\n",
    "#             print(sample[\"critique\"])\n",
    "#             print(\"\\nISSUE TYPE:\", sample[\"issue_type\"])\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading Llama-2: {str(e)}\")\n",
    "#         print(\"Falling back to another model...\")\n",
    "#         use_alternative = True\n",
    "# else:\n",
    "#     use_alternative = True\n",
    "\n",
    "# # Fallback to an alternative model if Llama-2 fails or user chose not to use it\n",
    "# if 'use_alternative' in locals() and use_alternative:\n",
    "#     print(\"\\n\\nUsing TinyLlama or Phi-2 instead...\")\n",
    "    \n",
    "#     # Define function to use TinyLlama\n",
    "#     def setup_tinyllama_model(device=\"cuda\", load_in_8bit=True):\n",
    "#         \"\"\"Set up the TinyLlama model for text generation.\"\"\"\n",
    "#         print(\"Loading TinyLlama model...\")\n",
    "        \n",
    "#         # Configure quantization if using CUDA\n",
    "#         if device == \"cuda\" and torch.cuda.is_available() and load_in_8bit:\n",
    "#             bnb_config = BitsAndBytesConfig(\n",
    "#                 load_in_8bit=True,\n",
    "#                 bnb_8bit_use_double_quant=True,\n",
    "#                 bnb_8bit_quant_type=\"nf4\",\n",
    "#                 bnb_8bit_compute_dtype=torch.float16\n",
    "#             )\n",
    "            \n",
    "#             # Load tokenizer and model - TinyLlama is open access\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "#             model = AutoModelForCausalLM.from_pretrained(\n",
    "#                 \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#                 device_map=\"auto\",\n",
    "#                 quantization_config=bnb_config,\n",
    "#                 trust_remote_code=True\n",
    "#             )\n",
    "#         else:\n",
    "#             # CPU or non-quantized loading\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "#             model = AutoModelForCausalLM.from_pretrained(\n",
    "#                 \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#                 device_map=\"auto\" if device == \"cuda\" else None,\n",
    "#                 trust_remote_code=True\n",
    "#             )\n",
    "        \n",
    "#         # Set padding token if not set\n",
    "#         if tokenizer.pad_token is None:\n",
    "#             tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "#         return tokenizer, model\n",
    "    \n",
    "#     # Define function to format TinyLlama prompt (same as Llama-2)\n",
    "#     def format_tinyllama_prompt(instruction, content=None):\n",
    "#         \"\"\"Format a prompt for TinyLlama.\"\"\"\n",
    "#         if content:\n",
    "#             return f\"<s>[INST] {instruction}\\n\\n{content} [/INST]\"\n",
    "#         else:\n",
    "#             return f\"<s>[INST] {instruction} [/INST]\"\n",
    "    \n",
    "#     # Run the same workflow with TinyLlama\n",
    "#     search_queries = [\n",
    "#         \"cat:cs.AI\",  # Artificial Intelligence\n",
    "#         \"cat:cs.CL\",  # Computational Linguistics\n",
    "#         \"cat:cs.LG\",  # Machine Learning\n",
    "#         \"cat:cs.CV\",  # Computer Vision\n",
    "#     ]\n",
    "    \n",
    "#     num_papers = int(input(\"Enter number of papers to download per category (5-10 recommended): \"))\n",
    "#     pdf_paths = download_arxiv_papers(search_queries, max_results=num_papers)\n",
    "#     paragraphs = extract_paragraphs_from_pdfs(pdf_paths)\n",
    "    \n",
    "#     tokenizer, model = setup_tinyllama_model()\n",
    "    \n",
    "#     num_examples = int(input(f\"Found {len(paragraphs)} paragraphs. How many examples to generate? \"))\n",
    "#     num_examples = min(num_examples, len(paragraphs))\n",
    "    \n",
    "#     dataset_splits = create_dataset(paragraphs, tokenizer, model, num_examples)\n",
    "    \n",
    "#     print(\"\\nDataset generation complete!\")\n",
    "#     print(\"You can now save this dataset to your Kaggle account.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T21:10:15.163178Z",
     "iopub.status.busy": "2025-04-15T21:10:15.162555Z",
     "iopub.status.idle": "2025-04-16T00:31:24.471301Z",
     "shell.execute_reply": "2025-04-16T00:31:24.470257Z",
     "shell.execute_reply.started": "2025-04-15T21:10:15.163151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of papers to download per category (5-10 recommended):  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download approximately 8 papers per category...\n",
      "Searching arXiv for: cat:cs.AI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edfd543cb1c4b09b55634b6aa423bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:cs.AI': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:cs.CL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6804aecc469453c80cd37ba5ce0b29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:cs.CL': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:cs.CV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e483141ef584bf19acb8e84ceb6bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:cs.CV': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:cs.LG\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594319f7e0a948c496d23ce0ef0ebb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:cs.LG': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:stat.ML\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6406aa8310524bb29ce92c466826a786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:stat.ML': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:cs.SE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b05ec05a6d0481d90bf75971fa71534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:cs.SE': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:physics.comp-ph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e833696fa8342e0b62c7b9f1bdbb7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:physics.comp-ph': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:q-bio.QM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9c081830d24e3b94143444d82fc02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:q-bio.QM': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:q-fin.ST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbe6eedbec44bf898ecb083c079db04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:q-fin.ST': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for: cat:cs.HC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3824e0326a64ea5ab3f90d2fe0f04ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers for 'cat:cs.HC': 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 80 papers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5d38a0836045408b4e8534aabcfee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing PDFs:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 3741 paragraphs from 80 PDFs\n",
      "Attempting to load Llama 3 8B with 4-bit quantization...\n",
      "Loading Llama 3 8B Instruct model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2434a8a8a9b243fcb4cc14736833f833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af6a3790c8d4f53b09672a6928ca7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6188fd138b740e7a2ae7456cf3165de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446c3c5653bc45529a74ef871e76c6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b938fd5e4d4773abd758cac03ffc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdbfb99896f4b1ba11a1fe85de07ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1276923cb5a345f481e7aaf0f0c33a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339d5114875245978b6922b6a7547ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e1d96078441fb9cf2732d43385e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5747499593274ce79f4c7087fe8cc6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed482820e6e4e4287f528bd32340b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a024084a4e884b0594d2d1abe69c66e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3741 paragraphs. How many examples would you like to generate?\n",
      "Recommended: At least 500 for good fine-tuning results, but more is better.\n",
      "Note: Starting with a smaller number (50-100) is good for testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of examples to generate (max 3741):  500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 500 examples. This will take some time...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ef7cb2d1814c9f8da2c648070d1d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating critiques:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:13:48.853159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744751629.104230      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744751629.177884      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2334611ef2b84a6ea78b99feaaf3f5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398b8dc43cbc40e59854a1c303b1b1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-formatted data saved to ./data\n",
      "Dataset saved to ./data\n",
      "Train set: 450 examples\n",
      "Validation set: 50 examples\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 3312)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/1297899901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;31m# 7. Analyze the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m \u001b[0manalyze_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/train_data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;31m# 8. Print instructions for saving and using the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/1297899901.py\u001b[0m in \u001b[0;36manalyze_dataset\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;31m# Count issue types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 3312)"
     ]
    }
   ],
   "source": [
    "# Llama 3 8B Dataset Generation for Research Paper Critiques\n",
    "# This script downloads real papers from arXiv and uses Meta's Llama 3 8B to create a critique dataset\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q arxiv==1.4.7 pymupdf==1.23.5 datasets==2.14.5 torch==2.1.0 \n",
    "!pip install -q transformers==4.36.0 accelerate==0.25.0 bitsandbytes==0.41.0 tqdm==4.66.1\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import arxiv\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p ./data\n",
    "!mkdir -p ./papers\n",
    "\n",
    "# Set your Hugging Face token directly\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_token\"  # Replace with your actual token\n",
    "\n",
    "# Function to download papers from arXiv\n",
    "def download_arxiv_papers(search_queries, max_results=10, output_dir=\"./papers\"):\n",
    "    \"\"\"Download papers from arXiv using the API.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    downloaded_papers = []\n",
    "    \n",
    "    for query in search_queries:\n",
    "        print(f\"Searching arXiv for: {query}\")\n",
    "        \n",
    "        # Create a search client\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        \n",
    "        # Download papers\n",
    "        for result in tqdm(search.results(), desc=f\"Downloading papers for '{query}'\"):\n",
    "            # Create a safe filename\n",
    "            paper_id = result.get_short_id()\n",
    "            filename = f\"{paper_id}.pdf\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Skip if already downloaded\n",
    "            if os.path.exists(filepath):\n",
    "                downloaded_papers.append(filepath)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Download the paper\n",
    "                result.download_pdf(dirpath=output_dir, filename=filename)\n",
    "                downloaded_papers.append(filepath)\n",
    "                \n",
    "                # Be nice to arXiv API\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {paper_id}: {str(e)}\")\n",
    "    \n",
    "    return downloaded_papers\n",
    "\n",
    "# Function to extract paragraphs from PDFs\n",
    "def extract_paragraphs_from_pdfs(pdf_paths, min_words=50, max_words=300):\n",
    "    \"\"\"Extract paragraphs from PDF files.\"\"\"\n",
    "    all_paragraphs = []\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_paths, desc=\"Processing PDFs\"):\n",
    "        try:\n",
    "            # Open the PDF\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            # Detect the starting page (skip front matter)\n",
    "            start_page = 0\n",
    "            for i in range(min(5, len(doc))):\n",
    "                text = doc[i].get_text().lower()\n",
    "                if any(marker in text for marker in [\"introduction\", \"background\", \"1. introduction\"]):\n",
    "                    start_page = i\n",
    "                    break\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(start_page, len(doc)):\n",
    "                page = doc[page_num]\n",
    "                blocks = page.get_text(\"blocks\")  # Get text as blocks which preserves layout better\n",
    "                \n",
    "                for block in blocks:\n",
    "                    # Block structure: (x0, y0, x1, y1, text, block_type, block_no)\n",
    "                    text = block[4].strip()\n",
    "                    \n",
    "                    # Clean up the text\n",
    "                    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "                    \n",
    "                    # Skip if empty or too short\n",
    "                    if not text or len(text.split()) < min_words:\n",
    "                        continue\n",
    "                        \n",
    "                    # Skip likely headers, footers, figure captions, etc.\n",
    "                    if re.match(r'^[0-9]+\\.?\\s*$', text):  # Just a number\n",
    "                        continue\n",
    "                    if re.match(r'^figure\\s+[0-9]+', text.lower()):  # Figure caption\n",
    "                        continue\n",
    "                    if re.match(r'^table\\s+[0-9]+', text.lower()):  # Table caption\n",
    "                        continue\n",
    "                    if re.match(r'^references', text.lower()):  # References section\n",
    "                        continue\n",
    "                    if text.isupper() and len(text.split()) < 15:  # ALL CAPS header\n",
    "                        continue\n",
    "                    \n",
    "                    # Handle paragraphs that are too long\n",
    "                    if len(text.split()) > max_words:\n",
    "                        words = text.split()\n",
    "                        chunks = []\n",
    "                        for i in range(0, len(words), max_words):\n",
    "                            chunk = ' '.join(words[i:i+max_words])\n",
    "                            chunks.append(chunk)\n",
    "                        all_paragraphs.extend(chunks)\n",
    "                    else:\n",
    "                        all_paragraphs.append(text)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Extracted {len(all_paragraphs)} paragraphs from {len(pdf_paths)} PDFs\")\n",
    "    return all_paragraphs\n",
    "\n",
    "# Function to set up Llama 3 model\n",
    "def setup_llama3_model(load_in_4bit=True):\n",
    "    \"\"\"Set up the Llama 3 8B model for text generation.\"\"\"\n",
    "    print(\"Loading Llama 3 8B Instruct model...\")\n",
    "    \n",
    "    # Configure quantization for memory efficiency\n",
    "    if load_in_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_quant_type=\"nf4\",\n",
    "            bnb_8bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                                             use_auth_token=os.environ.get(\"HF_TOKEN\"))\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "    )\n",
    "    \n",
    "    # Set padding token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to format Llama 3 prompt\n",
    "def format_llama3_prompt(instruction, content=None):\n",
    "    \"\"\"Format a prompt according to Llama 3's expected format.\"\"\"\n",
    "    if content:\n",
    "        return f\"<|begin_of_text|><|user|>\\n{instruction}\\n\\n{content}<|end_of_turn|>\\n<|assistant|>\"\n",
    "    else:\n",
    "        return f\"<|begin_of_text|><|user|>\\n{instruction}<|end_of_turn|>\\n<|assistant|>\"\n",
    "\n",
    "# Function to extract Llama 3 response\n",
    "def extract_llama3_response(generated_text):\n",
    "    \"\"\"Extract the model's response from the generated text.\"\"\"\n",
    "    response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"<|end_of_turn|>\" in response:\n",
    "        response = response.split(\"<|end_of_turn|>\")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Fallback functions for TinyLlama (in case Llama 3 fails)\n",
    "def format_tinyllama_prompt(instruction, content=None):\n",
    "    if content:\n",
    "        return f\"<|user|>\\n{instruction}\\n\\n{content}\\n<|assistant|>\"\n",
    "    else:\n",
    "        return f\"<|user|>\\n{instruction}\\n<|assistant|>\"\n",
    "\n",
    "def extract_tinyllama_response(generated_text):\n",
    "    response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"<|user|>\" in response:\n",
    "        response = response.split(\"<|user|>\")[0].strip()\n",
    "    return response\n",
    "\n",
    "# Fallback functions for Phi-2 (in case Llama 3 fails)\n",
    "def format_phi2_prompt(instruction, content=None):\n",
    "    if content:\n",
    "        return f\"Instruct: {instruction}\\n\\nInput: {content}\\n\\nOutput:\"\n",
    "    else:\n",
    "        return f\"Instruct: {instruction}\\n\\nOutput:\"\n",
    "\n",
    "def extract_phi2_response(generated_text):\n",
    "    if \"Output:\" in generated_text:\n",
    "        response = generated_text.split(\"Output:\")[-1].strip()\n",
    "        return response\n",
    "    else:\n",
    "        # As a fallback, return everything after the prompt\n",
    "        return generated_text.split(\"Input:\")[-1].strip()\n",
    "\n",
    "# Function to generate a critique for a paragraph\n",
    "def generate_critique(tokenizer, model, paragraph, issue_type=None, format_prompt=format_llama3_prompt, extract_response=extract_llama3_response):\n",
    "    \"\"\"\n",
    "    Generate a critique for a paragraph using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Model tokenizer\n",
    "        model: Loaded model\n",
    "        paragraph: Research paper paragraph to critique\n",
    "        issue_type: Specific issue type to focus on (optional)\n",
    "        format_prompt: Function to format the prompt for the specific model\n",
    "        extract_response: Function to extract the response from the specific model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with paragraph, critique, and issue_type\n",
    "    \"\"\"\n",
    "    # Define issue types\n",
    "    issue_types = [\n",
    "        \"missing_evidence\",\n",
    "        \"logical_contradiction\",\n",
    "        \"unclear_argument\",\n",
    "        \"poor_citation\",\n",
    "        \"grammar_spelling\",\n",
    "        \"undefined_terminology\",\n",
    "        \"statistical_error\",\n",
    "        \"methodology_issue\",\n",
    "        \"unsubstantiated_claim\",\n",
    "        \"structural_issue\",\n",
    "        \"well_written\"  # Include examples with no issues\n",
    "    ]\n",
    "    \n",
    "    # Select a random issue type if not specified\n",
    "    if issue_type is None:\n",
    "        issue_type = random.choice(issue_types)\n",
    "    \n",
    "    # Create instruction\n",
    "    instruction = f\"\"\"You are an expert academic reviewer with years of experience reviewing research papers.\n",
    "Analyze the following paragraph from a real research paper and provide a detailed critique.\n",
    "Focus on identifying issues related to: {issue_type.replace('_', ' ')}.\n",
    "If you genuinely find no issues, explain why the paragraph is well-written instead.\n",
    "Your critique should be specific, actionable, and professional.\n",
    "\n",
    "Provide only the critique - do not include any introductory text or explanations about your role.\"\"\"\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = format_prompt(instruction, paragraph)\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        \n",
    "        # Extract response\n",
    "        critique = extract_response(generated_text)\n",
    "        \n",
    "        # Create result\n",
    "        result = {\n",
    "            \"paragraph\": paragraph,\n",
    "            \"critique\": critique,\n",
    "            \"issue_type\": issue_type\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating critique: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(paragraphs, tokenizer, model, num_examples, output_path=\"./data\", \n",
    "                  format_prompt=format_llama3_prompt, extract_response=extract_llama3_response):\n",
    "    \"\"\"\n",
    "    Create a dataset of paragraph-critique pairs.\n",
    "    \n",
    "    Args:\n",
    "        paragraphs: List of research paper paragraphs\n",
    "        tokenizer: Model tokenizer\n",
    "        model: Loaded model\n",
    "        num_examples: Number of examples to generate\n",
    "        output_path: Path to save the dataset\n",
    "        format_prompt: Function to format prompts for the specific model\n",
    "        extract_response: Function to extract responses from the specific model\n",
    "    \n",
    "    Returns:\n",
    "        Dataset splits (train/val)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Ensure we don't try to generate more examples than paragraphs\n",
    "    num_examples = min(num_examples, len(paragraphs))\n",
    "    \n",
    "    # Shuffle paragraphs\n",
    "    random.shuffle(paragraphs)\n",
    "    \n",
    "    # Create progress bar\n",
    "    with tqdm(total=num_examples, desc=\"Generating critiques\") as pbar:\n",
    "        for i in range(num_examples):\n",
    "            # Get paragraph\n",
    "            paragraph = paragraphs[i]\n",
    "            \n",
    "            # Generate critique\n",
    "            example = generate_critique(tokenizer, model, paragraph, \n",
    "                                       format_prompt=format_prompt, \n",
    "                                       extract_response=extract_response)\n",
    "            \n",
    "            if example:\n",
    "                examples.append(example)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Clear CUDA cache occasionally\n",
    "            if i % 5 == 4:  # Clear cache frequently for memory efficiency\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset = Dataset.from_list(examples)\n",
    "    \n",
    "    # Split into train/validation sets (90/10)\n",
    "    splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    # Save the dataset\n",
    "    splits[\"train\"].to_json(os.path.join(output_path, \"train_data.json\"))\n",
    "    splits[\"test\"].to_json(os.path.join(output_path, \"val_data.json\"))\n",
    "    \n",
    "    # Format for Mistral\n",
    "    format_for_mistral(splits, output_path)\n",
    "    \n",
    "    print(f\"Dataset saved to {output_path}\")\n",
    "    print(f\"Train set: {len(splits['train'])} examples\")\n",
    "    print(f\"Validation set: {len(splits['test'])} examples\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Function to format for Mistral\n",
    "def format_for_mistral(dataset, output_path):\n",
    "    \"\"\"Format dataset specifically for Mistral fine-tuning.\"\"\"\n",
    "    formatted_train = []\n",
    "    \n",
    "    for example in dataset[\"train\"]:\n",
    "        formatted_train.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Review and critique the following research paper paragraph. Identify any logical issues, missing evidence, contradictions, or other problems:\\n\\n{example['paragraph']}\"},\n",
    "                {\"role\": \"assistant\", \"content\": example['critique']}\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    formatted_val = []\n",
    "    \n",
    "    for example in dataset[\"test\"]:\n",
    "        formatted_val.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Review and critique the following research paper paragraph. Identify any logical issues, missing evidence, contradictions, or other problems:\\n\\n{example['paragraph']}\"},\n",
    "                {\"role\": \"assistant\", \"content\": example['critique']}\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # Save formatted data\n",
    "    with open(os.path.join(output_path, \"train_mistral_format.json\"), \"w\") as f:\n",
    "        json.dump(formatted_train, f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(output_path, \"val_mistral_format.json\"), \"w\") as f:\n",
    "        json.dump(formatted_val, f, indent=2)\n",
    "    \n",
    "    print(f\"Mistral-formatted data saved to {output_path}\")\n",
    "\n",
    "# Function to analyze dataset diversity\n",
    "def analyze_dataset(dataset_path):\n",
    "    \"\"\"Analyze the diversity and quality of the generated dataset.\"\"\"\n",
    "    # Load dataset\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Count issue types\n",
    "    issue_counts = {}\n",
    "    for example in data:\n",
    "        issue_type = example['issue_type']\n",
    "        issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1\n",
    "    \n",
    "    # Calculate average lengths\n",
    "    paragraph_lengths = [len(example['paragraph'].split()) for example in data]\n",
    "    critique_lengths = [len(example['critique'].split()) for example in data]\n",
    "    \n",
    "    avg_paragraph_length = sum(paragraph_lengths) / len(paragraph_lengths)\n",
    "    avg_critique_length = sum(critique_lengths) / len(critique_lengths)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset Analysis:\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    print(f\"Average paragraph length: {avg_paragraph_length:.1f} words\")\n",
    "    print(f\"Average critique length: {avg_critique_length:.1f} words\")\n",
    "    print(\"\\nIssue type distribution:\")\n",
    "    for issue_type, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"- {issue_type}: {count} ({count/len(data)*100:.1f}%)\")\n",
    "\n",
    "# MAIN EXECUTION\n",
    "\n",
    "# 1. Define search queries for arXiv - diverse academic fields\n",
    "search_queries = [\n",
    "    \"cat:cs.AI\",  # Artificial Intelligence\n",
    "    \"cat:cs.CL\",  # Computational Linguistics\n",
    "    \"cat:cs.CV\",  # Computer Vision\n",
    "    \"cat:cs.LG\",  # Machine Learning\n",
    "    \"cat:stat.ML\",  # Machine Learning (Statistics)\n",
    "    \"cat:cs.SE\",  # Software Engineering\n",
    "    \"cat:physics.comp-ph\",  # Computational Physics\n",
    "    \"cat:q-bio.QM\",  # Quantitative Methods in Biology\n",
    "    \"cat:q-fin.ST\",  # Statistical Finance\n",
    "    \"cat:cs.HC\"  # Human-Computer Interaction\n",
    "]\n",
    "\n",
    "# Ask user for number of papers to download per category\n",
    "num_papers = int(input(\"Enter number of papers to download per category (5-10 recommended): \"))\n",
    "print(f\"Will download approximately {num_papers} papers per category...\")\n",
    "\n",
    "# 2. Download papers from arXiv\n",
    "pdf_paths = download_arxiv_papers(search_queries, max_results=num_papers)\n",
    "print(f\"Downloaded {len(pdf_paths)} papers\")\n",
    "\n",
    "# 3. Extract paragraphs from PDFs\n",
    "paragraphs = extract_paragraphs_from_pdfs(pdf_paths)\n",
    "\n",
    "# 4. Set up model with 4-bit quantization for memory efficiency\n",
    "format_prompt = format_llama3_prompt\n",
    "extract_response = extract_llama3_response\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to load Llama 3 8B with 4-bit quantization...\")\n",
    "    tokenizer, model = setup_llama3_model(load_in_4bit=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Llama 3: {str(e)}\")\n",
    "    print(\"This might be due to Hugging Face token access issues or memory constraints.\")\n",
    "    \n",
    "    fallback = input(\"Do you want to try a fallback model instead? (y/n): \")\n",
    "    if fallback.lower() == 'y':\n",
    "        # Offer alternative models\n",
    "        print(\"\\nAlternative models:\")\n",
    "        print(\"1. TinyLlama (1.1B parameters)\")\n",
    "        print(\"2. Phi-2 (2.7B parameters)\")\n",
    "        alt_choice = input(\"Choose a fallback model (1-2): \")\n",
    "        \n",
    "        if alt_choice == \"1\":\n",
    "            # TinyLlama fallback\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            print(\"Loading TinyLlama model...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            # Use TinyLlama prompt format\n",
    "            format_prompt = format_tinyllama_prompt\n",
    "            extract_response = extract_tinyllama_response\n",
    "        elif alt_choice == \"2\":\n",
    "            # Phi-2 fallback\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            print(\"Loading Phi-2 model...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"microsoft/phi-2\",\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            # Use Phi-2 prompt format\n",
    "            format_prompt = format_phi2_prompt\n",
    "            extract_response = extract_phi2_response\n",
    "        else:\n",
    "            print(\"Invalid choice. Exiting.\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(\"Exiting program.\")\n",
    "        exit(1)\n",
    "\n",
    "# 5. Ask user for dataset size\n",
    "max_size = len(paragraphs)\n",
    "print(f\"Found {max_size} paragraphs. How many examples would you like to generate?\")\n",
    "print(f\"Recommended: At least 500 for good fine-tuning results, but more is better.\")\n",
    "print(f\"Note: Starting with a smaller number (50-100) is good for testing.\")\n",
    "num_examples = int(input(f\"Enter number of examples to generate (max {max_size}): \"))\n",
    "num_examples = min(num_examples, max_size)\n",
    "\n",
    "# 6. Generate dataset\n",
    "print(f\"Generating {num_examples} examples. This will take some time...\")\n",
    "dataset_splits = create_dataset(paragraphs, tokenizer, model, num_examples, \n",
    "                               format_prompt=format_prompt, \n",
    "                               extract_response=extract_response)\n",
    "\n",
    "# 7. Analyze the dataset\n",
    "analyze_dataset(\"./data/train_data.json\")\n",
    "\n",
    "# 8. Print instructions for saving and using the dataset\n",
    "print(\"\\nDataset generation complete!\")\n",
    "print(\"To save this dataset to your Kaggle account:\")\n",
    "print(\"1. Click on the 'Data' tab in the right panel\")\n",
    "print(\"2. Under 'Output', click '+ Save All'\")\n",
    "print(\"3. Enter a name for your dataset (e.g., 'llama3-paper-critique-data')\")\n",
    "print(\"4. Click 'Save'\")\n",
    "print(\"\\nYou can then use this dataset in a new notebook for fine-tuning Mistral\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"\\nHere's a sample from the generated dataset:\")\n",
    "with open(\"./data/train_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    sample = data[0]\n",
    "    print(\"\\nPARAGRAPH:\")\n",
    "    print(sample[\"paragraph\"])\n",
    "    print(\"\\nCRITIQUE:\")\n",
    "    print(sample[\"critique\"])\n",
    "    print(\"\\nISSUE TYPE:\", sample[\"issue_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:45:33.149051Z",
     "iopub.status.busy": "2025-04-15T20:45:33.148269Z",
     "iopub.status.idle": "2025-04-15T20:49:31.221327Z",
     "shell.execute_reply": "2025-04-15T20:49:31.220601Z",
     "shell.execute_reply.started": "2025-04-15T20:45:33.149026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash_attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash_attn) (2.1.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash_attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (4.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash_attn) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->flash_attn) (1.3.0)\n",
      "Building wheels for collected packages: flash_attn\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for flash_attn (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for flash_attn\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for flash_attn\n",
      "Failed to build flash_attn\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash_attn)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T19:53:42.720864Z",
     "iopub.status.idle": "2025-04-15T19:53:42.721079Z",
     "shell.execute_reply": "2025-04-15T19:53:42.720988Z",
     "shell.execute_reply.started": "2025-04-15T19:53:42.720978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Replace 'output' with your actual output directory name\n",
    "shutil.make_archive('/kaggle/working/data', 'zip', '/kaggle/working/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T19:53:42.722133Z",
     "iopub.status.idle": "2025-04-15T19:53:42.722477Z",
     "shell.execute_reply": "2025-04-15T19:53:42.722308Z",
     "shell.execute_reply.started": "2025-04-15T19:53:42.722293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Display clickable link to download\n",
    "FileLink('/kaggle/working/output_dir.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T01:11:33.154790Z",
     "iopub.status.busy": "2025-04-16T01:11:33.154460Z",
     "iopub.status.idle": "2025-04-16T01:11:48.467072Z",
     "shell.execute_reply": "2025-04-16T01:11:48.465858Z",
     "shell.execute_reply.started": "2025-04-16T01:11:33.154764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
      "featuretools 1.31.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\n",
      "nilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion because of the following error (look up to see its traceback):\nFailed to import diffusers.loaders.single_file because of the following error (look up to see its traceback):\nNo module named 'torch.sparse._triton_ops_meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/loaders/single_file.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_transformers_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .single_file_utils import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mSingleFileComponentError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/loaders/single_file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m from ..schedulers import (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersAutoQuantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersAutoQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorchao\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchAoHfQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/torchao/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorchao_quantizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchAoHfQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/torchao/torchao_quantizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torchao_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantize_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from torchao.quantization import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mautoquant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torchao.kernel import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mint_scaled_matmul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msafe_int_mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/kernel/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbsr_triton_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbsr_dense_addmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mint_scaled_matmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_int_mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/kernel/bsr_triton_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_triton_ops_meta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_triton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_triton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.sparse._triton_ops_meta'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mimage_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineImageInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVaeImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mloaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFromSingleFileMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIPAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionLoraLoaderMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextualInversionLoaderMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProjection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNet2DConditionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    923\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import diffusers.loaders.single_file because of the following error (look up to see its traceback):\nNo module named 'torch.sparse._triton_ops_meta'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/454092834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextHistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBestOfNSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_diffusers_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_wandb_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_xpu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m from .models import (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/extras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbest_of_n_sampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBestOfNSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/extras/best_of_n_sampler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSUPPORTED_ARCHITECTURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModelWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_diffusers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     from .modeling_sd_base import (\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mDDPOPipelineOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mDDPOSchedulerOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/models/modeling_sd_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDIMScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNet2DConditionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttnProcsLayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoRAAttnProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    923\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion because of the following error (look up to see its traceback):\nFailed to import diffusers.loaders.single_file because of the following error (look up to see its traceback):\nNo module named 'torch.sparse._triton_ops_meta'"
     ]
    }
   ],
   "source": [
    "# Mistral 7B Fine-Tuning Script for Research Paper Critique\n",
    "# This script fine-tunes Mistral 7B on your research paper critique dataset\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers==4.36.0 peft==0.8.0 trl==0.7.4 accelerate==0.25.0\n",
    "!pip install -q bitsandbytes==0.41.0 datasets==2.14.5 scipy==1.11.4 tqdm==4.66.1\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Or v0.1 if preferred\n",
    "TRAIN_FILE = \"/kaggle/working/data/train_mistral_format.json\"\n",
    "VAL_FILE = \"/kaggle/working/data/val_mistral_format.json\"\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 2  # Adjust based on GPU memory\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "# Function to load datasets\n",
    "def load_datasets(train_file, val_file):\n",
    "    \"\"\"Load train and validation datasets formatted for Mistral.\"\"\"\n",
    "    logger.info(f\"Loading datasets from {train_file} and {val_file}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_dataset = load_dataset('json', data_files=train_file)['train']\n",
    "        val_dataset = load_dataset('json', data_files=val_file)['train']\n",
    "        \n",
    "        logger.info(f\"Loaded {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")\n",
    "        \n",
    "        # Clean up any special tokens in the data\n",
    "        def clean_text(example):\n",
    "            for i, message in enumerate(example['messages']):\n",
    "                if 'content' in message:\n",
    "                    # Remove model-specific tokens like <|eot_id|>\n",
    "                    message['content'] = message['content'].replace(\"<|eot_id|>\", \"\").strip()\n",
    "                    example['messages'][i] = message\n",
    "            return example\n",
    "        \n",
    "        train_dataset = train_dataset.map(clean_text)\n",
    "        val_dataset = val_dataset.map(clean_text)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading datasets: {e}\")\n",
    "        raise\n",
    "\n",
    "# Set up Mistral model\n",
    "def setup_mistral_model():\n",
    "    \"\"\"Set up the Mistral model for fine-tuning.\"\"\"\n",
    "    logger.info(f\"Loading {MODEL_NAME}\")\n",
    "    \n",
    "    # Configure quantization for memory efficiency\n",
    "    if LOAD_IN_4BIT:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_skip_modules=None,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Set padding token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Set up LoRA configuration\n",
    "def get_lora_config():\n",
    "    \"\"\"Get LoRA configuration for Mistral model.\"\"\"\n",
    "    # Define target modules for Mistral\n",
    "    target_modules = [\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \n",
    "        \"up_proj\", \n",
    "        \"down_proj\"\n",
    "    ]\n",
    "    \n",
    "    # Create LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    return lora_config\n",
    "\n",
    "# Set up training arguments\n",
    "def get_training_args():\n",
    "    \"\"\"Create training arguments.\"\"\"\n",
    "    # Create a timestamp for the output directory\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = os.path.join(\"./models\", f\"mistral-7b-critique_{timestamp}\")\n",
    "    \n",
    "    # Create training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.001,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load datasets\n",
    "    train_dataset, val_dataset = load_datasets(TRAIN_FILE, VAL_FILE)\n",
    "\n",
    "    # Setup model\n",
    "    model, tokenizer = setup_mistral_model()\n",
    "    \n",
    "    # Prepare model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Get LoRA configuration\n",
    "    lora_config = get_lora_config()\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Get training arguments\n",
    "    training_args = get_training_args()\n",
    "    \n",
    "    # Set up the SFT trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=lora_config,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        packing=False,  # Disable packing for more stable training\n",
    "        dataset_text_field=\"messages\"  # Use messages field for ChatML format\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    output_dir = os.path.join(training_args.output_dir, \"final_model\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    logger.info(f\"Training complete. Model saved to {output_dir}\")\n",
    "    \n",
    "    # Save a smaller file with just the LoRA adapters\n",
    "    adapter_output_dir = os.path.join(training_args.output_dir, \"lora_adapters\")\n",
    "    model.save_pretrained(adapter_output_dir)\n",
    "    logger.info(f\"LoRA adapters saved to {adapter_output_dir}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# To run inference with the fine-tuned model\n",
    "def run_inference(model_path, prompt):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "# paragraph = \"Your research paragraph here...\"\n",
    "# prompt = f\"Review and critique the following research paper paragraph. Identify any logical issues, missing evidence, contradictions, or other problems:\\n\\n{paragraph}\"\n",
    "# response = run_inference(\"./models/mistral-7b-critique_TIMESTAMP/final_model\", prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:34:58.171786Z",
     "iopub.status.busy": "2025-04-16T03:34:58.171112Z",
     "iopub.status.idle": "2025-04-16T03:36:33.669689Z",
     "shell.execute_reply": "2025-04-16T03:36:33.668384Z",
     "shell.execute_reply.started": "2025-04-16T03:34:58.171759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "datasets 3.5.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\n",
      "featuretools 1.31.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 54 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/1476843139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading tokenizer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 )\n\u001b[0;32m--> 787\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Otherwise we have to be creative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2026\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2028\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2029\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m             raise OSError(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/tokenization_llama_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[0;32m--> 124\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_slow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 54 column 3"
     ]
    }
   ],
   "source": [
    "# Kaggle notebook for testing your fine-tuned Mistral model\n",
    "!pip install -q transformers==4.36.0 accelerate==0.25.0 bitsandbytes==0.41.0 tqdm==4.66.1\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set your model path (replace with your actual dataset name)\n",
    "model_path = \"/kaggle/input/mistral-llama/transformers/default/1/content/mistral-critique-model\"  # Adjust this path\n",
    "\n",
    "# Create offload directory if needed\n",
    "os.makedirs(\"./offload_folder\", exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with proper quantization\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True  # Use 8-bit quantization for efficient inference\n",
    ")\n",
    "\n",
    "# Function to generate critiques\n",
    "def critique_paragraph(paragraph, max_new_tokens=1024):\n",
    "    \"\"\"Generate a research paper critique for a given paragraph\"\"\"\n",
    "    # Format the prompt\n",
    "    prompt = f\"Review and critique the following research paper paragraph. Identify any logical issues, missing evidence, contradictions, or other problems:\\n\\n{paragraph}\"\n",
    "    \n",
    "    # Format as chat for Mistral\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Apply chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test examples\n",
    "test_paragraphs = [\n",
    "    \"\"\"The integration of machine learning algorithms into healthcare settings has shown promising results, reducing diagnostic errors by 30% in preliminary studies. This improvement can be attributed to the ability of these systems to process vast amounts of patient data and identify patterns that might be missed by human practitioners. Furthermore, the implementation of these technologies has been well-received by medical professionals, with 85% reporting increased confidence in their diagnostic decisions when supported by AI tools.\"\"\",\n",
    "    \n",
    "    \"\"\"Recent advances in natural language processing have enabled more accurate sentiment analysis in social media posts, with reported accuracy rates exceeding 90%. This represents a significant improvement over previous methods and opens new avenues for understanding public opinion at scale.\"\"\",\n",
    "    \n",
    "    \"\"\"Our experiment showed a statistically significant effect (p < 0.05) of the new treatment on patient recovery times. The treatment group showed a 45% reduction in recovery time compared to the control group, suggesting this approach could revolutionize care standards.\"\"\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "for i, paragraph in enumerate(test_paragraphs):\n",
    "    print(f\"\\n\\n=== Test Example {i+1} ===\")\n",
    "    print(f\"\\nParagraph:\\n{paragraph}\\n\")\n",
    "    print(f\"Generating critique...\")\n",
    "    critique = critique_paragraph(paragraph)\n",
    "    print(f\"\\nCritique:\\n{critique}\")\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 305484,
     "modelInstanceId": 284642,
     "sourceId": 340379,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
